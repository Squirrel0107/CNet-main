{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HF&ML components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "#This input list contains N*N dictionary, which is the value of cosine similarity between node and node\n",
    "def node_reassemble_cosine(df, threshold):\n",
    "    verify_set = []\n",
    "    #initialize\n",
    "    for i in range(len(df)):\n",
    "        #Pass Sets\n",
    "        verify_set.append(set([key for key, value in df[i].items() if value >= threshold]))\n",
    "    return verify_set\n",
    "\n",
    "#This has two inputs: the original dataset (processed train data X) and the NRC adjustment standard, \n",
    "#which is to use the original dataset for NRC processing (for each features)\n",
    "#NRC is abbreviation of Node Reassemble Calibrate\n",
    "def node_reassemble_intensity(X, NRC):\n",
    "    result = []\n",
    "    ID = []\n",
    "    for i in range(len(X)):\n",
    "        ID.append(i)\n",
    "    #After the corresponding df, use dictionary corresponding\n",
    "    dic_X = dict(zip(ID, X))\n",
    "    for i in range(len(X)):\n",
    "        #strongest neighbors, and their features\n",
    "        element_dic_X = [value for key, value in dic_X.items() if key in NRC[i]]\n",
    "        #Add up their features as denominator\n",
    "        sum_element_dic_X = sum(element_dic_X)\n",
    "        #Get the value of the target\n",
    "        target = [value for key, value in dic_X.items() if key == i][0]\n",
    "        #Node reassemble\n",
    "        result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
    "    return result\n",
    "    \n",
    "\n",
    "def get_NRC_X(path, feature_list, threshold):\n",
    "    \n",
    "    #read csv\n",
    "    df = pd.read_csv(path,names=feature_list)\n",
    "    #remove NA\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    \n",
    "    #Take the available features (assume the first is the ID, the last is the Label)\n",
    "    X = pd.DataFrame(df, columns = feature_list[1:-1])\n",
    "    X = np.squeeze(np.asarray(X))\n",
    "    \n",
    "    #Cosine Similarity\n",
    "    Cos_X = cosine_similarity(X)\n",
    "    \n",
    "    #Prepare to make a corresponding table (dictionary)\n",
    "    ID = []\n",
    "    for i in range(len(Cos_X)):\n",
    "        ID.append(i)\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(Cos_X)):\n",
    "        result.append(dict(zip(ID, Cos_X[i])))\n",
    "        \n",
    "    NRC = node_reassemble_cosine(result, threshold)\n",
    "    NRC_df = node_reassemble_intensity(X, NRC)\n",
    "    \n",
    "    return NRC_df\n",
    "    \n",
    "    \n",
    "#inputs two-dimensional, get the original citation data\n",
    "def get_citation_ori_data(path, feature_list):\n",
    "    #read csv\n",
    "    df = pd.read_csv(path,names=feature_list)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    #Take available features (assuming the first is ID, the last is Label)\n",
    "    X2 = pd.DataFrame(df, columns = feature_list[:])\n",
    "    X2 = np.squeeze(np.asarray(X2))\n",
    "    return X2\n",
    "\n",
    "#[dic_x] inputs are two-dimensional, assemble the original citation data, a node corresponds to a citation list\n",
    "def get_citation(df):\n",
    "    dic_x = {}\n",
    "    for i in range(len(df)):\n",
    "        #Make an adjacent matrix used to find the citation relationship (cited group corresponding to each citing)\n",
    "        #If citing is not currently in dictionary\n",
    "        if df[i][0] not in dic_x.keys():\n",
    "            #Create a list\n",
    "            dic_x[df[i][0]]=[]\n",
    "            #Add the current target cited node to the set corresponding to the citing node\n",
    "            dic_x[df[i][0]].append(df[i][1])\n",
    "        #If citing is currently in dictionary\n",
    "        elif df[i][0] in dic_x.keys():\n",
    "            #Add the current target cited node directly to the set corresponding to the citing node\n",
    "            dic_x[df[i][0]].append(df[i][1])\n",
    "        else:\n",
    "            print('Error')\n",
    "            break\n",
    "    return dic_x\n",
    "\n",
    "#[df] inputs are two-dimensional, but we only need to use the citing data, specify the first feature as citing, and count the number of times it is cited\n",
    "def get_citation_df_for_hop(path, feature_list):\n",
    "    #read csv\n",
    "    df = pd.read_csv(path,names=feature_list)\n",
    "    df = df.fillna(0)\n",
    "    df2 = df.groupby(feature_list[0]).size().reset_index(name='Count')\n",
    "    X2 = pd.DataFrame(df2, columns = [feature_list[0],\"Count\"])\n",
    "    X2 = np.squeeze(np.asarray(X2))\n",
    "    return X2\n",
    "\n",
    "#[dic_citation] inputs are two-dimensional, but we only need to use the citing data to count the number of citations\n",
    "def get_hop1_citation(df):\n",
    "    dic_citation = {}\n",
    "    for i in range(len(df)):\n",
    "        dic_citation[df[i][0]]=df[i][1]\n",
    "    return dic_citation\n",
    "\n",
    "\n",
    "def get_hopk_citation(df, dic_x, dic_citation):\n",
    "    dic_citation_next = {}\n",
    "    for i in range(len(df)):\n",
    "        #Current_node_citation_values\n",
    "        #Get the Citation table, and the \"those\" nodes cited by each target node\n",
    "        cur_ = [value for key, value in dic_x.items() if key == df[i][0]]\n",
    "        cur_ = np.squeeze(cur_)\n",
    "\n",
    "        #Current_node_citation_values_transform\n",
    "        #Get the hop k-1 table and replace those nodes obtained in the previous step with their corresponding values\n",
    "        cur_t = [value for key, value in dic_citation.items() if key in cur_]\n",
    "        #sum\n",
    "        new_ = sum(cur_t)\n",
    "        #Update\n",
    "        dic_citation_next[df[i][0]]=new_\n",
    "    return dic_citation_next\n",
    "\n",
    "def get_hopk_pre(df2, dic_x, dic_citation, k):\n",
    "    if k == 1:\n",
    "        return dic_citation\n",
    "    elif k > 1:\n",
    "        for i in range(k-1):\n",
    "            if i == 0:\n",
    "                cur_result = get_hopk_citation(df2, dic_x, dic_citation)\n",
    "            elif i > 0:\n",
    "                cur_result = get_hopk_citation(df2, dic_x, cur_result)\n",
    "        return cur_result\n",
    "    \n",
    "def get_NRC_hopk(path, feature_list, threshold):\n",
    "    \n",
    "    #read csv\n",
    "    df = pd.read_csv(path,names=feature_list)\n",
    "    #remove NA\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    \n",
    "    # Take available features (assuming the first is ID, the last is Label)\n",
    "    X = pd.DataFrame(df, columns = feature_list[1:-1])\n",
    "    X = np.squeeze(np.asarray(X))\n",
    "    \n",
    "    #Cosine Similarity\n",
    "    Cos_X = cosine_similarity(X)\n",
    "    \n",
    "    #(dictionary)\n",
    "    ID = []\n",
    "    for i in range(len(Cos_X)):\n",
    "        ID.append(i)\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(Cos_X)):\n",
    "        result.append(dict(zip(ID, Cos_X[i])))\n",
    "        \n",
    "    NRC = node_reassemble_cosine(result, threshold)\n",
    "#     NRC_df = node_reassemble_intensity(X, NRC)\n",
    "    \n",
    "    return NRC\n",
    "\n",
    "def get_hopk(path, feature_list, threshold):\n",
    "    #[original inputs]\n",
    "    df1 = get_citation_ori_data(path, feature_list)\n",
    "    #[dic_x] is converted into, {node A:[node B, node C, ...]} method, each node, corresponding to those other nodes referenced by itself\n",
    "    dic_x = get_citation(df1)\n",
    "\n",
    "    #[df] Get the front DATA of the hop 1 comparison table, which is used as the benchmark df for all hop corresponding tables\n",
    "    df2 = get_citation_df_for_hop(path, feature_list)\n",
    "    #[dic_citation] Obtain the hop 1 corresponding table through the benchmark df (only the first time the hop is obtained and used as initialize data, and then self-update through hop k)\n",
    "    dic_citation = get_hop1_citation(df2)\n",
    "    #[hop k table]\n",
    "    hop_k = get_hopk_pre(df2, dic_x, dic_citation, threshold)\n",
    "    return hop_k\n",
    "\n",
    "def get_pro(df, hop1, hop_NRC, feature_list):\n",
    "    # Take available features (assuming the first is ID, the last is Label)\n",
    "    X_hop = pd.DataFrame(df, columns = feature_list[0:1])\n",
    "    X_hop = np.squeeze(np.asarray(X_hop))\n",
    "\n",
    "\n",
    "    #The node name corresponding to each number\n",
    "    ID = []\n",
    "    for i in range(len(X_hop)):\n",
    "        ID.append(i)\n",
    "    #After the corresponding df, use dictionary corresponding\n",
    "    dic_hop = dict(zip(ID, X_hop))\n",
    "    dic_hop_bp = copy.deepcopy(dic_hop)\n",
    "\n",
    "    #Find the hop-k value of the neighbor corresponding to the target node\n",
    "    for i in range(len(dic_hop_bp)):\n",
    "        if dic_hop[i] in hop1.keys():\n",
    "            element_dic_hop = [value for key, value in dic_hop.items() if key in hop_NRC[i]]\n",
    "            hop1_value = [value for key, value in hop1.items() if key in element_dic_hop]\n",
    "            dic_hop_bp[i]=hop1_value\n",
    "        else:\n",
    "            dic_hop_bp[i]=[0]\n",
    "\n",
    "\n",
    "    #target node itself corresponds to the value of hop-k\n",
    "    target = []\n",
    "    for i in range(len(dic_hop_bp)):\n",
    "        if dic_hop[i] in hop1.keys():\n",
    "            target.append(hop1[dic_hop[i]])\n",
    "        else:\n",
    "            target.append(0)\n",
    "\n",
    "\n",
    "    #This is the NP multiplier, if there is no reference information, it will be replaced by 1\n",
    "    result = []\n",
    "    for i in range(len(dic_hop_bp)):\n",
    "        if target[i] == 0:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            tn = (target[i]/sum(dic_hop_bp[i]))*len(dic_hop_bp[i])\n",
    "            result.append(tn)\n",
    "    return result\n",
    "\n",
    "def extractDigits(lst):\n",
    "    return [[el] for el in lst]\n",
    "\n",
    "def get_pro_NRC(path, feature_list, threshold):\n",
    "    \n",
    "    #read csv\n",
    "    df = pd.read_csv(path,names=feature_list)\n",
    "    #remove NA\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    \n",
    "    # Take available features (assuming the first is ID, the last is Label)\n",
    "    X = pd.DataFrame(df, columns = feature_list[1:-1])\n",
    "    X = np.squeeze(np.asarray(X))\n",
    "    \n",
    "    #Cosine Similarity\n",
    "    Cos_X = cosine_similarity(X)\n",
    "    \n",
    "    #(dictionary)\n",
    "    ID = []\n",
    "    for i in range(len(Cos_X)):\n",
    "        ID.append(i)\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(Cos_X)):\n",
    "        result.append(dict(zip(ID, Cos_X[i])))\n",
    "        \n",
    "    NRC = node_reassemble_cosine(result, threshold)\n",
    "#     pro_ = get_pro(df, hopk, NRC, feature_list2)\n",
    "#     pro_ = extractDigits(pro_)\n",
    "    NRC_df = node_reassemble_intensity(X, NRC)\n",
    "#     pro_NRC_df = np.multiply(pro_,NRC_df)\n",
    "    return NRC_df\n",
    "\n",
    "def get_pro_NRC_output(path, feature_list, threshold, output_name):\n",
    "    #NRC calculates T1 (without ID, Label)\n",
    "    result = get_pro_NRC(path, feature_list, threshold)\n",
    "    np.savetxt(output_name, result, delimiter=\",\")\n",
    "    #Read T1(NRC, ID to be added, Label)\n",
    "    df = pd.read_csv(output_name,names =feature_list[1:-1])\n",
    "    #Read T0(raw data, including ID, Label)\n",
    "    df2 = pd.read_csv(path,names =feature_list)\n",
    "    #Insert ID into the first feature\n",
    "    df.insert(loc=0, column=feature_list[0], value=df2[feature_list[0]])\n",
    "    #Insert Label to the last feature\n",
    "    df.insert(loc=len(df2.columns)-1, column=feature_list[-1], value=df2[feature_list[-1]])\n",
    "    df = df.fillna(0)\n",
    "    df.to_csv(output_name, index=False,header=False,sep ='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Cora) - Running Calibrate Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [] #Store features including ID, Label\n",
    "feat_list = [] #A list to hold the feature vector for each node\n",
    "label_list = [] #Used to store the list of categories corresponding to each node\n",
    "node_map = {} #Recode the node\n",
    "label_map = {} #Mapping labels to numbers\n",
    "        \n",
    "with open('cora/cora.content') as f1:\n",
    "    for i,each_sample in enumerate(f1.readlines()): #Iterate over the features of each sample\n",
    "        sample_clean = each_sample.strip().split()\n",
    "        #Extract the features of each sample, where the first and last elements are the sample name and corresponding label\n",
    "        feat_list.append(sample_clean[1:-1]) \n",
    "        df_list.append(sample_clean[:]) \n",
    "        #Map node names to node numbers\n",
    "        node_map[sample_clean[0]]=i\n",
    "        label = sample_clean[-1]\n",
    "        if label not in label_map.keys():\n",
    "            #Convert labels to numbers\n",
    "            label_map[label] = len(label_map)\n",
    "        label_list.append(label_map[label])\n",
    "    feat_list = np.asarray(feat_list,dtype=np.float64)\n",
    "    df_list = np.asarray(df_list)\n",
    "    label_list = np.asarray(label_list,dtype=np.int64)\n",
    "    \n",
    "pd.DataFrame(df_list).to_csv('cora_content.csv',index=False, header = False)\n",
    "\n",
    "cora_names = []\n",
    "for i in range(len(feat_list[0])):\n",
    "    cora_names.append(str(i))\n",
    "\n",
    "cora_names.insert(0,'paper_id')\n",
    "cora_names.insert(len(feat_list[0])+1,'class_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [] #Store features including ID, Label\n",
    "feat_list = [] #A list to hold the feature vector for each node\n",
    "label_list = [] #Used to store the list of categories corresponding to each node\n",
    "node_map = {} #Recode the node\n",
    "label_map = {} #Mapping labels to numbers\n",
    "        \n",
    "with open('cora/cora.cites') as f1:\n",
    "    for i,each_sample in enumerate(f1.readlines()): #Iterate over the features of each sample\n",
    "        sample_clean = each_sample.strip().split()\n",
    "        #Extract the features of each sample, where the first and last elements are the sample name and corresponding label\n",
    "        feat_list.append(sample_clean[1:-1]) \n",
    "        df_list.append(sample_clean[:]) \n",
    "        #Map node names to node numbers\n",
    "        node_map[sample_clean[0]]=i\n",
    "        label = sample_clean[-1]\n",
    "        if label not in label_map.keys():\n",
    "            #Convert labels to numbers\n",
    "            label_map[label] = len(label_map)\n",
    "        label_list.append(label_map[label])\n",
    "    feat_list = np.asarray(feat_list,dtype=np.float64)\n",
    "    df_list = np.asarray(df_list)\n",
    "    label_list = np.asarray(label_list,dtype=np.int64)\n",
    "\n",
    "pd.DataFrame(df_list).to_csv('cora_cites.csv',index=False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Node] The first is ID, the last is Label\n",
    "feature_list_cora_content=cora_names\n",
    "#[Citation] Citation and Citation - 2D\n",
    "feature_list_cora_cites=[\"Citing\",\"Cited\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n",
      "<ipython-input-1-5385ffac08c0>:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result.append(((target/sum_element_dic_X)*len(element_dic_X)*target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:16:46.207919\n"
     ]
    }
   ],
   "source": [
    "#calculating running time\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "#[Node]Data storage\n",
    "path = 'cora_content.csv'\n",
    "\n",
    "#[Node]Set Cosine similarity size\n",
    "threshold = [0.95, 0.90, 0.80, 0.70, 0.60, 0.50]\n",
    "\n",
    "#[Citation]Data storage \n",
    "path2 = 'cora_cites.csv'\n",
    "\n",
    "\n",
    "for i in threshold:\n",
    "    #hop-k\n",
    "    hopk = get_hopk(path2, feature_list_cora_cites, 1)\n",
    "    #return cora_nc_cos_*.csv\n",
    "    get_pro_NRC_output(path,feature_list_cora_content, i, 'cora_nc_cos_'+\\\n",
    "                                     str(i)+'.csv')\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
